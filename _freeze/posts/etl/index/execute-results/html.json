{
  "hash": "51e12a05b1f6415a9967526eaab76d0e",
  "result": {
    "markdown": "---\ntitle: \"ETL: Extract, Transform, Load\"\nauthor: \"Bailie Wynbelt\"\ndate: \"2023-10-17\"\ncategories: [Python, SQL, AWS, Validation]\nimage: \"etl.png\"\nformat: html\n---\n\nWhat can I say... I LOVE THE ETL PROCESS. It is so fun and groovy. This document contains code that was written for my data engineering class. Some of the code was provided by the professor - however it is noted when the code was not written by me. \n\nThe goal of this homework was to take a semi-structured non-normalized CSV file and turn it into a set of normalized tables that was then pushed to a **postgres** database hosted on **AWS**. Throughout the homework, I created a **database schema** and created tables with primary and foreign keys within the database. The database contains tables cases, judges, and casetype. These tables contain information about judge name, political party, gender, race, case year, casetype, etc. \n\nThis data could be used for a wide range of uses. For example, analysts could use the final summary table (that is created at the end of this document) to look at how political party inpacts court decisions.\n\n## Bring in data and explore.\nThe goal of this section is to import the data and gain a general overview of the data. \n\nWe cannot forget our import statements!\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport psycopg2\nimport numpy as np\nfrom pandas._libs.lib import count_level_2d\n```\n:::\n\n\nFirst, we need to read in the data to get started.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#written by professor\ncases = pd.read_csv('https://docs.google.com/spreadsheets/d/1AWLK06JOlSKImgoHNTbj7oXR5mRfsL2WWeQF6ofMq1g/gviz/tq?tqx=out:csv')\n```\n:::\n\n\nLets look at the first and last 5 rows of the data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprint(cases.head())\nprint(cases.tail())\n```\n:::\n\n\nNow we know what columns our dataframe contains...but what are the data types of each? Are they what we could expect?\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ncases.dtypes\n```\n:::\n\n\n## Create functions that will help execute our queries and checks/validation faster.\nThe following code was written by the professor - however it is crucial to the checks and execution of the database creation.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# run_query function\ndef run_query(query_string):\n\n conn, cur = get_conn_cur() # get connection and cursor\n\n cur.execute(query_string) # executing string as before\n\n my_data = cur.fetchall() # fetch query data as before\n\n # here we're extracting the 0th element for each item in cur.description\n colnames = [desc[0] for desc in cur.description]\n\n cur.close() # close\n conn.close() # close\n\n return(colnames, my_data) # return column names AND data\n\n# Column name function for checking out what's in a table\ndef get_column_names(table_name): # arguement of table_name\n conn, cur = get_conn_cur() # get connection and cursor\n\n # Now select column names while inserting the table name into the WERE\n column_name_query = \"\"\"SELECT column_name FROM information_schema.columns\n    WHERE table_name = '%s' \"\"\" %table_name\n\n cur.execute(column_name_query) # exectue\n my_data = cur.fetchall() # store\n\n cur.close() # close\n conn.close() # close\n\n return(my_data) # return\n\n# Check table_names\ndef get_table_names():\n  conn, cur = get_conn_cur() # get connection and cursor\n\n  # query to get table names\n  table_name_query = \"\"\"SELECT table_name FROM information_schema.tables\n       WHERE table_schema = 'public' \"\"\"\n\n  cur.execute(table_name_query) # execute\n  my_data = cur.fetchall() # fetch results\n\n  cur.close() #close cursor\n  conn.close() # close connection\n\n  return(my_data) # return your fetched results\n\n# make sql_head function\ndef sql_head(table_name):\n conn, cur = get_conn_cur() # get connection and cursor\n\n # Now select column names while inserting the table name into the WERE\n head_query = \"\"\"SELECT * FROM %s LIMIT 5; \"\"\" %table_name\n\n cur.execute(head_query) # exectue\n colnames = [desc[0] for desc in cur.description] # get column names\n my_data = cur.fetchall() # store first five rows\n\n cur.close() # close\n conn.close() # close\n\n df = pd.DataFrame(data = my_data, columns = colnames) # make into df\n\n return(df) # return\n\ndef my_drop_table(tab_name):\n  conn, cur = get_conn_cur()\n  tq = \"\"\"DROP TABLE IF EXISTS %s CASCADE;\"\"\" %tab_name\n  cur.execute(tq)\n  conn.commit()\n```\n:::\n\n\n## Make cases table and push to database.\nFor this section I will be creating the first table of the schema: **cases**. This table will contain the columns case_id, case_year, category_name, casetype_id, judge_id, libcon_name. The data will be linked to the judge table by the judge_id column.\n\nFirst we need to make a foreign key to connect with the judge table.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Make judge_id in cases\ncases['judge_id'] = pd.factorize(cases['judge_name'])[0].astype(str)\ncases\n```\n:::\n\n\nSelect necessary columns to make cases_df.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ncases_df = cases[['case_id','case_year', 'category_name', 'casetype_id', 'judge_id', 'libcon_name']]\ncases_df\n```\n:::\n\n\nThis is creating a database connection - confidential information such as user and password are not included for safety:)\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n#written by professor\ndef get_conn_cur(): # define function name and arguments (there aren't any)\n conn = psycopg2.connect(\n    host=\"\",\n    database=\"\",\n    user=\"\",\n    password=\"\",\n    port='5432'\n    )\n cur = conn.cursor()   # Make a cursor after\n return(conn, cur)   # Return both the connection and the cursor\n```\n:::\n\n\nMake table in database.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n#make table\ntq = \"\"\"CREATE TABLE cases (\n     case_id INT PRIMARY KEY,\n     case_year INT NOT NULL ,\n     category_name VARCHAR(255) NOT NULL,\n     casetype_id INT NOT NULL,\n     judge_id INT NOT NULL,\n     libcon_name VARCHAR(255) NOT NULL\n     );\"\"\"\n```\n:::\n\n\nExecute and commit table!\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nconn, cur = get_conn_cur()\ncur.execute(tq)\nconn.commit()\n```\n:::\n\n\nCheck column names to make sure everything looks correct.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nget_column_names(table_name='cases')\n```\n:::\n\n\nConvert into tuple.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ncases_tuple = cases_df.to_numpy();\ncases_tuple[:,1] = np.vectorize(lambda x: str(x))(cases_tuple[:,1])\ndata_tups = [tuple(x) for x in cases_tuple]\n```\n:::\n\n\nwrite SQL string to add local data to database.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\niq = \"\"\"INSERT INTO cases(case_id, case_year, category_name, casetype_id, judge_id, libcon_name) VALUES(%s, %s, %s, %s, %s, %s);\"\"\"\n```\n:::\n\n\nCommit and check complete table.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n#Execute the string\nconn, cur = get_conn_cur()\ncur.executemany(iq, data_tups)\n\n#Commit and check\nconn.commit()\nconn.close()\n\n#check\nsql_head(table_name='cases')\n```\n:::\n\n\n## Make a rollup table\nThe process used to create the cases table is the same process that was used to create the other connecting tables (judges and casetype). To avoid repition, I will be skipping this code. However, I will be using the remaining code that was written. This has the goal of creating a summary table analysts could use to look at case decisions and political party type.\n\nFirst, I made a groupby called cases_rollup. This groups by party_name and category name and aggregates the count and sum of libcon_id.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ncases_rollup = cases.groupby(['category_name', 'party_name'])['libcon_id'].agg(['sum', 'count']).reset_index()\ncases_rollup\n\ncases_rollup.reset_index()\n```\n:::\n\n\n#Rename columns to be consistant with new values.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# rename your columns now. Keep the first to the same but call the last two 'total_cases' and 'num_lib_decisions'\ncases_rollup.rename(columns={'sum': 'num_lib_decisions', 'count': 'total_cases'}, inplace = True)\ncases_rollup\n```\n:::\n\n\nNow, I created a new column called 'percent_liberal'. This calculates the percentage of decisions that were liberal in nature.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ncases_rollup['percent_liberal'] = round((cases_rollup['num_lib_decisions'] / cases_rollup['total_cases']) * 100)\ncases_rollup\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}