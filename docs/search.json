[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I’m Bailie! I am currently a senior pursuing a Bachelor of Science in Information Science with an emphasis in Data Science. My overall mission is to tackle challenging real-world problems by harnessing the power of data. My main skills fall in the realm of data wrangling, manipulation, analysis, visualization, and management with the overall goal of driving decision-making.\n\nIn addition to my academic and professional endeavors, I strive for diversity and inclusion in the tech community, with a particular interest in the queer tech community. I believe in the importance of fostering an inclusive environment where everyone’s talents and perspectives are valued!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bailie Wynbelt",
    "section": "",
    "text": "Hello! My name is Bailie.\nI am currently pursuing a Bachelor of Science in Information Science with an emphasis in Data Science. My expected graduation data is May 2024.\nIn my spare time, you can find me hiking around beautiful southern Arizona or crocheting!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Bailie Wynbelt",
    "section": "Education",
    "text": "Education\nUniversity of Arizona | Tucson, AZ | B.S in Information Science | August 2020 - May 2024"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Bailie Wynbelt",
    "section": "Experience",
    "text": "Experience\nSchool of Information | Undergraduate Section Leader | January 2024 - present\nNational Phenology Network | Systems Assistant | September 2023 - December 2023\nVirga Labs | Data Science Intern | May 2023 - September 2023\nArizona Youth Partnership | Summer Intern (Analyst) | May 2023 - August 2023\nSchool of Natural Resources and the Environment | Data Science Research Assistant | January 2023 - May 2023"
  },
  {
    "objectID": "posts/etl/index.html",
    "href": "posts/etl/index.html",
    "title": "ETL: Extract, Transform, Load",
    "section": "",
    "text": "The ETL (Extract, Transform, and Load), is a useful process for taking unuseable data and making it useable! This document contains code that was written for my data engineering class. Some of the code was provided by the professor - however it is noted when the code was not written by me.\nThe goal of this homework was to take a semi-structured non-normalized CSV file and turn it into a set of normalized tables that was then pushed to a postgres database hosted on AWS. Throughout the homework, I created a database schema and created tables with primary and foreign keys within the database. The database contains tables cases, judges, and casetype. These tables contain information about judge name, political party, gender, race, case year, casetype, etc.\nThis data could be used for a wide range of uses. For example, analysts could use the final summary table (that is created at the end of this document) to look at how political party inpacts court decisions."
  },
  {
    "objectID": "posts/etl/index.html#bring-in-data-and-explore.",
    "href": "posts/etl/index.html#bring-in-data-and-explore.",
    "title": "ETL: Extract, Transform, Load",
    "section": "Bring in data and explore.",
    "text": "Bring in data and explore.\nThe goal of this section is to import the data and gain a general overview of the data.\nWe cannot forget our import statements!\n\nimport pandas as pd\nimport psycopg2\nimport numpy as np\nfrom pandas._libs.lib import count_level_2d\n\nFirst, we need to read in the data to get started.\n\n#written by professor\ncases = pd.read_csv('https://docs.google.com/spreadsheets/d/1AWLK06JOlSKImgoHNTbj7oXR5mRfsL2WWeQF6ofMq1g/gviz/tq?tqx=out:csv')\n\nLets look at the first and last 5 rows of the data.\n\nprint(cases.head())\nprint(cases.tail())\n\nNow we know what columns our dataframe contains…but what are the data types of each? Are they what we could expect?\n\ncases.dtypes"
  },
  {
    "objectID": "posts/etl/index.html#create-functions-that-will-help-execute-our-queries-and-checksvalidation-faster.",
    "href": "posts/etl/index.html#create-functions-that-will-help-execute-our-queries-and-checksvalidation-faster.",
    "title": "ETL: Extract, Transform, Load",
    "section": "Create functions that will help execute our queries and checks/validation faster.",
    "text": "Create functions that will help execute our queries and checks/validation faster.\nThe following code was written by the professor - however it is crucial to the checks and execution of the database creation.\n\n# run_query function\ndef run_query(query_string):\n\n conn, cur = get_conn_cur() # get connection and cursor\n\n cur.execute(query_string) # executing string as before\n\n my_data = cur.fetchall() # fetch query data as before\n\n # here we're extracting the 0th element for each item in cur.description\n colnames = [desc[0] for desc in cur.description]\n\n cur.close() # close\n conn.close() # close\n\n return(colnames, my_data) # return column names AND data\n\n# Column name function for checking out what's in a table\ndef get_column_names(table_name): # arguement of table_name\n conn, cur = get_conn_cur() # get connection and cursor\n\n # Now select column names while inserting the table name into the WERE\n column_name_query = \"\"\"SELECT column_name FROM information_schema.columns\n    WHERE table_name = '%s' \"\"\" %table_name\n\n cur.execute(column_name_query) # exectue\n my_data = cur.fetchall() # store\n\n cur.close() # close\n conn.close() # close\n\n return(my_data) # return\n\n# Check table_names\ndef get_table_names():\n  conn, cur = get_conn_cur() # get connection and cursor\n\n  # query to get table names\n  table_name_query = \"\"\"SELECT table_name FROM information_schema.tables\n       WHERE table_schema = 'public' \"\"\"\n\n  cur.execute(table_name_query) # execute\n  my_data = cur.fetchall() # fetch results\n\n  cur.close() #close cursor\n  conn.close() # close connection\n\n  return(my_data) # return your fetched results\n\n# make sql_head function\ndef sql_head(table_name):\n conn, cur = get_conn_cur() # get connection and cursor\n\n # Now select column names while inserting the table name into the WERE\n head_query = \"\"\"SELECT * FROM %s LIMIT 5; \"\"\" %table_name\n\n cur.execute(head_query) # exectue\n colnames = [desc[0] for desc in cur.description] # get column names\n my_data = cur.fetchall() # store first five rows\n\n cur.close() # close\n conn.close() # close\n\n df = pd.DataFrame(data = my_data, columns = colnames) # make into df\n\n return(df) # return\n\ndef my_drop_table(tab_name):\n  conn, cur = get_conn_cur()\n  tq = \"\"\"DROP TABLE IF EXISTS %s CASCADE;\"\"\" %tab_name\n  cur.execute(tq)\n  conn.commit()"
  },
  {
    "objectID": "posts/etl/index.html#make-cases-table-and-push-to-database.",
    "href": "posts/etl/index.html#make-cases-table-and-push-to-database.",
    "title": "ETL: Extract, Transform, Load",
    "section": "Make cases table and push to database.",
    "text": "Make cases table and push to database.\nFor this section I will be creating the first table of the schema: cases. This table will contain the columns case_id, case_year, category_name, casetype_id, judge_id, libcon_name. The data will be linked to the judge table by the judge_id column.\nFirst we need to make a foreign key to connect with the judge table.\n\n# Make judge_id in cases\ncases['judge_id'] = pd.factorize(cases['judge_name'])[0].astype(str)\ncases\n\nSelect necessary columns to make cases_df.\n\ncases_df = cases[['case_id','case_year', 'category_name', 'casetype_id', 'judge_id', 'libcon_name']]\ncases_df\n\nThis is creating a database connection - confidential information such as user and password are not included for safety:)\n\n#written by professor\ndef get_conn_cur(): # define function name and arguments (there aren't any)\n conn = psycopg2.connect(\n    host=\"\",\n    database=\"\",\n    user=\"\",\n    password=\"\",\n    port='5432'\n    )\n cur = conn.cursor()   # Make a cursor after\n return(conn, cur)   # Return both the connection and the cursor\n\nMake table in database.\n\n#make table\ntq = \"\"\"CREATE TABLE cases (\n     case_id INT PRIMARY KEY,\n     case_year INT NOT NULL ,\n     category_name VARCHAR(255) NOT NULL,\n     casetype_id INT NOT NULL,\n     judge_id INT NOT NULL,\n     libcon_name VARCHAR(255) NOT NULL\n     );\"\"\"\n\nExecute and commit table!\n\nconn, cur = get_conn_cur()\ncur.execute(tq)\nconn.commit()\n\nCheck column names to make sure everything looks correct.\n\nget_column_names(table_name='cases')\n\nConvert into tuple.\n\ncases_tuple = cases_df.to_numpy();\ncases_tuple[:,1] = np.vectorize(lambda x: str(x))(cases_tuple[:,1])\ndata_tups = [tuple(x) for x in cases_tuple]\n\nwrite SQL string to add local data to database.\n\niq = \"\"\"INSERT INTO cases(case_id, case_year, category_name, casetype_id, judge_id, libcon_name) VALUES(%s, %s, %s, %s, %s, %s);\"\"\"\n\nCommit and check complete table.\n\n#Execute the string\nconn, cur = get_conn_cur()\ncur.executemany(iq, data_tups)\n\n#Commit and check\nconn.commit()\nconn.close()\n\n#check\nsql_head(table_name='cases')"
  },
  {
    "objectID": "posts/etl/index.html#make-a-rollup-table",
    "href": "posts/etl/index.html#make-a-rollup-table",
    "title": "ETL: Extract, Transform, Load",
    "section": "Make a rollup table",
    "text": "Make a rollup table\nThe process used to create the cases table is the same process that was used to create the other connecting tables (judges and casetype). To avoid repetition, I will be skipping this code. However, I will be using the remaining code that was written. This has the goal of creating a summary table analysts could use to look at case decisions and political party type.\nFirst, I made a groupby called cases_rollup. This groups by party_name and category name and aggregates the count and sum of libcon_id.\n\ncases_rollup = cases.groupby(['category_name', 'party_name'])['libcon_id'].agg(['sum', 'count']).reset_index()\ncases_rollup\n\ncases_rollup.reset_index()\n\n#Rename columns to be consistent with new values.\n\n# rename your columns now. Keep the first to the same but call the last two 'total_cases' and 'num_lib_decisions'\ncases_rollup.rename(columns={'sum': 'num_lib_decisions', 'count': 'total_cases'}, inplace = True)\ncases_rollup\n\nNow, I created a new column called ‘percent_liberal’. This calculates the percentage of decisions that were liberal in nature.\n\ncases_rollup['percent_liberal'] = round((cases_rollup['num_lib_decisions'] / cases_rollup['total_cases']) * 100)\ncases_rollup"
  },
  {
    "objectID": "posts/iris_python/index.html",
    "href": "posts/iris_python/index.html",
    "title": "Python Data Wrangling Workshop",
    "section": "",
    "text": "The following is a workshop that I created to teach beginner programmers an introduction to data wrangling in Python using Pandas!"
  },
  {
    "objectID": "posts/iris_python/index.html#data-science-more-fun-less-pain",
    "href": "posts/iris_python/index.html#data-science-more-fun-less-pain",
    "title": "Python Data Wrangling Workshop",
    "section": "Data Science: more fun, less pain",
    "text": "Data Science: more fun, less pain\nBut wait…you might be wondering what is pandas and how can we use it in Data Science?\nPython is a dynamic language that can be used for a variety of problems these ranging from Software Enginnering to Data Science. Today, we are going to focus on how to use Python, specifically the pandas package, to wrangle, analyze, and visualize data. The pandas package is a collection of functions that allows us to easily manipulate, summarize, and visualize data. In this lesson, we will use the pandas package and the iris dataset to wrangle data, create summary statistics, and develop appealing visualizations.\n\nDescriptive statisitcs and visualization with pandas\nBailie Wynbelt\nwynbeltb@arizona.edu\n2024-02-04\n\n\n#import statements\nimport pandas as pd\n\nimport numpy as np\n\nfrom plotnine import *\n\nNow that we have imported all the required packages, we can read in the data.\nTo do this we will use the read_csv() function from pandas\n\n#import iris data set\n#This is a dataframe, meaning it is a two-dimentional structure.\ncsv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' #get csv url\n\niris =  pd.read_csv(csv_url, names = ['sepal_length','sepal_width','petal_length','petal_width','species'])\n\niris"
  },
  {
    "objectID": "posts/iris_python/index.html#explore-the-data",
    "href": "posts/iris_python/index.html#explore-the-data",
    "title": "Python Data Wrangling Workshop",
    "section": "Explore the Data",
    "text": "Explore the Data\nWe have imported the pandas package and the dataset! Now we can start exploring the dataset and working on summarizing the data.\nSome of my favorite ways of exploring the dataset include the following:\n\n#Gives the number of rows and columns in the format (rows, columns)\niris.shape()\n\n\n#Shows the name ofthe column, number of non-null values, and the datatype.\niris.info()\n\n\n#Returns the first 5 rows\niris.head()\n\n\n#Returns the last 5 rows\niris.tail()\n\nWe have imported the pandas package/dataset and have explored the data! Now we can start working on summarizing the data."
  },
  {
    "objectID": "posts/iris_python/index.html#summarizing-the-data",
    "href": "posts/iris_python/index.html#summarizing-the-data",
    "title": "Python Data Wrangling Workshop",
    "section": "Summarizing the data",
    "text": "Summarizing the data\nHow can we create descriptive statistics for the iris dataset? -Means -Standard errors -For each trait -For each species\n\n#Quick statisitic summary of data\niris.describe()\n\n\n#Find the mean of sepal length for all species\niris_mean = iris[\"sepal_length\"].mean()\n\niris_mean\n\n\n#Find the standard deviation of sepal length for all species\niris_std = iris[\"sepal_length\"].std()\n\niris_std\n\nWe have found the mean and standard deviation for sepal_length. However, there are also different species, so what if there are differences in traits between species?\nTo explore this question we can use the groupby function.\n\n#find the mean for each species for each trait (column)\niris_mean = iris.groupby('species').mean()\n\niris_mean\n\nWhat if we also want to find the standard deviation for each trait for each species?\nWe can use the agg function alongside the groupby function.\n\n#find the mean and standard deviation for each species for each trait (column)\niris_stats = iris.groupby('species').agg(['mean', 'std'])\n\niris_stats\n\nGreat! Now we have a descriptive statistics for all species and traits. Similar steps can be taken if you just want to find descriptive statistics for one column or trait. Lets explore how to do this below.\n\n#find the mean per species for sepal length\niris_mean = iris.groupby('species')['sepal_length'].mean()\n\niris_mean\n\n\n#find the mean and standard deviation for sepal length of each species\niris_stats = iris.groupby('species')['sepal_length'].agg(['mean', 'std'])\n\niris_stats"
  },
  {
    "objectID": "posts/iris_python/index.html#plotting-data",
    "href": "posts/iris_python/index.html#plotting-data",
    "title": "Python Data Wrangling Workshop",
    "section": "Plotting Data",
    "text": "Plotting Data\nSo far we have:\n\nread the data\nexplored the data\ncreated descriptive statistics\n\nWe are now read to visualize the data! In R, people commonly use ggplot2 to visualize datasets. In python, we will use plot9 which essentially functions similarly to ggplot2 and outputs similar visualizations\nFirst, we need to create a general plot and then state what type of plot we want.\nWe are going to create a scatterplot displaying sepal length on the x-axis and sepal width on the y-axis.\n\n(ggplot(iris, aes(x = \"sepal_length\", y = \"sepal_width\")) \n+ geom_point())\n\nIt looks like there is two distinct groupings within our scatterplot, lets explore this more by adding an extra argument into the aes() function.\n\n(ggplot(iris, aes(x = \"sepal_length\", y = \"sepal_width\", color = \"species\")) \n+ geom_point())\n\nLastly, we can better visualize the differences between species by faceting and creating a linear model. This can be done by adding the facet_wrap and stat_smooth argument.\nA facet wrap is dividing the graph into sections based on a particular variable. In our case, it is species.\nLastly, we can beautify the graph by adding labels and a title.\n\n(ggplot(iris, aes(x = \"sepal_length\", \n                  y = \"sepal_width\", \n                  color = \"species\")) +\n        geom_point() +\n        facet_wrap(\"~species\") +\n        stat_smooth(method = \"lm\", color = \"black\") +\n        labs(x = \"Sepal Legnth\",\n            y = \"Sepal Width\",\n            title = \"Sepal Length VS Sepal Width of Iris Species\"))"
  },
  {
    "objectID": "posts/iris_python/index.html#explore-the-dataset",
    "href": "posts/iris_python/index.html#explore-the-dataset",
    "title": "Python Data Wrangling Workshop",
    "section": "Explore the dataset",
    "text": "Explore the dataset\nEmploy one or two of the exploratory functions we used before. Use which ever ones are your favorite!\n\n#explore the data\npenguins.head()\n\n\npenguins.tail()\n\n\npenguins.describe()"
  },
  {
    "objectID": "posts/iris_python/index.html#find-descriptive-statistics",
    "href": "posts/iris_python/index.html#find-descriptive-statistics",
    "title": "Python Data Wrangling Workshop",
    "section": "Find Descriptive Statistics",
    "text": "Find Descriptive Statistics\nWe want to find descriptive statistics for the dataset! For your challenge, find the mean and standard deviation of the “body_mass_g” column per species\n\npenguins_stats = penguins.groupby('species')['body_mass_g'].agg(['mean', 'std'])\n\npenguins_stats"
  },
  {
    "objectID": "posts/iris_python/index.html#visualization",
    "href": "posts/iris_python/index.html#visualization",
    "title": "Python Data Wrangling Workshop",
    "section": "Visualization",
    "text": "Visualization\nFor your next challenge… try creating a visualization that displays body mass on the x-axis vs flipper length on the y-axis with color differentiated by sex (color in the aes() argument) and the graph faceted by species. Add labels that you see fit to the graph.\nSteps to be taken 1) Create a scatterplot (geom_point) with “body_mass_g” on the x-axis and “flipper_length_mm” on the y-axis. Don’t forgot to add aes(color = ) set to “sex” ! 2) Add a facet_wrap by “species” 3) Add labels with the labs() argument\n\npenguins_stats = penguins.groupby('species')['body_mass_g'].agg(['mean', 'std'])\n\npenguins_stats"
  },
  {
    "objectID": "posts/palmer_R/index.html",
    "href": "posts/palmer_R/index.html",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "",
    "text": "I used the free Palmer Penguins data set in R to conduct analysis and create visualizations"
  },
  {
    "objectID": "posts/palmer_R/index.html#reading-tidyverse",
    "href": "posts/palmer_R/index.html#reading-tidyverse",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Reading tidyverse",
    "text": "Reading tidyverse\nThe following lines of code is reading in the tidyverse package so specific functions can be used to analyze the data.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/palmer_R/index.html#loading-in-the-palmer-penguin-data-set",
    "href": "posts/palmer_R/index.html#loading-in-the-palmer-penguin-data-set",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Loading in the palmer penguin data set",
    "text": "Loading in the palmer penguin data set\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.2.3\n\npenguin_data <- penguins"
  },
  {
    "objectID": "posts/palmer_R/index.html#gaining-a-general-oversight-of-the-data",
    "href": "posts/palmer_R/index.html#gaining-a-general-oversight-of-the-data",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Gaining a general oversight of the data",
    "text": "Gaining a general oversight of the data\nThe head() and tail() functions give a general oversight of what the data is comprised of in terms of rows and columns.\n\nhead(penguin_data)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\ntail(penguin_data)\n\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Chinstrap Dream            45.7          17            195    3650 fema…  2009\n2 Chinstrap Dream            55.8          19.8          207    4000 male   2009\n3 Chinstrap Dream            43.5          18.1          202    3400 fema…  2009\n4 Chinstrap Dream            49.6          18.2          193    3775 male   2009\n5 Chinstrap Dream            50.8          19            210    4100 male   2009\n6 Chinstrap Dream            50.2          18.7          198    3775 fema…  2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-tidying",
    "href": "posts/palmer_R/index.html#data-tidying",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data tidying",
    "text": "Data tidying\nThe following lines of code are tidying the “penguin_data” data set. The code is removing any values in the data set that have an NA and creating a new data set called “clean_penguin_data”\n\nclean_penguin_data <- penguin_data %>%\n  drop_na()"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-manipulation",
    "href": "posts/palmer_R/index.html#data-manipulation",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data manipulation",
    "text": "Data manipulation\nThe following lines of code are manipulation the “clean_penguin_data” data set. The code is converting millimeters to centimeters, and grams to milligrams. New columns will be created in the clean_penguin_data\n\nclean_penguin_data <- clean_penguin_data %>%\n  mutate(bill_length_cm = bill_length_mm/10,\n         bill_depth_cm = bill_depth_mm/10,\n         flipper_length_cm = flipper_length_mm/10,\n         body_mass_mg = body_mass_g/1000)"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-analysis-of-islands",
    "href": "posts/palmer_R/index.html#data-analysis-of-islands",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data analysis of islands",
    "text": "Data analysis of islands\nBelow are lines of code that are summarizing the “clean_penguin_data” based on specific attributes. The following is grouping the data by island, and calculating the mean of bill length, bill depth, flipper length, and body mass. This data is added to a new data frame called “island_penguin_summary”.\n\nisland_penguin_summary <- clean_penguin_data %>%\n  group_by(island) %>%\n  summarize(mean_bill_length_mm = mean(bill_length_mm),\n            mean_bill_depth_mm = mean(bill_depth_mm),\n            mean_flipper_length_mm = mean(flipper_length_mm),\n            mean_body_mass_g = mean(body_mass_g))"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-analysis-of-sex",
    "href": "posts/palmer_R/index.html#data-analysis-of-sex",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data analysis of sex",
    "text": "Data analysis of sex\nBelow are lines of code that are summarizing the “clean_penguin_data” based on specific attributes. The following is grouping the data by sex, and calculating the mean of bill length, bill depth, flipper length, and body mass. This data is added to a new data frame called “sex_penguin_summary”.\n\nsex_penguin_summary <- clean_penguin_data %>%\n  group_by(sex) %>%\n  summarize(mean_bill_length_mm = mean(bill_length_mm),\n            mean_bill_depth_mm = mean(bill_depth_mm),\n            mean_flipper_length_mm = mean(flipper_length_mm),\n            mean_body_mass_g = mean(body_mass_g))"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-analysis-of-species",
    "href": "posts/palmer_R/index.html#data-analysis-of-species",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data analysis of species",
    "text": "Data analysis of species\nBelow are lines of code that are summarizing the “clean_penguin_data” based on specific attributes. The following is grouping the data by species, and calculating the mean of bill length, bill depth, flipper length, and body mass. This data is added to a new data frame called “species_penguin_summary”.\n\nspecies_penguin_summary <- clean_penguin_data %>%\n  group_by(species) %>%\n  summarize(bill_length_mm = mean(bill_length_mm),\n            mean_bill_depth_mm = mean(bill_depth_mm),\n            mean_flipper_length_mm = mean(flipper_length_mm),\n            mean_body_mass_g = mean(body_mass_g))"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-visualization-of-body-mass-vs-flipper-length-of-penguins",
    "href": "posts/palmer_R/index.html#data-visualization-of-body-mass-vs-flipper-length-of-penguins",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data visualization of body mass vs flipper length of penguins",
    "text": "Data visualization of body mass vs flipper length of penguins\nThe code lines below are utilizing the ggplot package to visualize the “clean_penguin_data”. This data is a scatter plot with body mass on the x-axis and flipper length on the y-axis, with the color of data points being differentiated by color. Additionally, the graph is facted by species of the penguins\n\nggplot(data = clean_penguin_data,\n       mapping = aes(x = body_mass_g, \n                     y = flipper_length_mm,\n                     color = sex)) +\n  geom_point() +\n  facet_wrap(~species) +\n  labs(x = \"Body Mass of Penguins (g)\",\n       y = \"Flipper Length of Penguins (mm)\",\n       title = \"Body Mass (g) VS Flipper Length (mm) of Penguins\") +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/wrangling_R/index.html",
    "href": "posts/wrangling_R/index.html",
    "title": "Data Wrangling, Updates, and Validation",
    "section": "",
    "text": "The following code is a representation of my work in School of Natural Resources. I was using data that was collected in 2017 and we wanted to run new analyses.\nThe data consists of rodents samples that were collected in the southwest. The goal of this data was to display what rodents are eating and how they are diet partitioning.\nThis document shows the wrangling, update, and validation of the large data using the package Worldflora to find updated taxonomic names.\nIt is important to note that the data will not be displayed in this document. The data is large and confidental. However, my code is still in the useable format and results in a dataset with updated taxonomic names and codes!\nInstall and load packages.\n\nlibrary(WorldFlora)\nlibrary(tidyverse)\n\nDownload and remember WFO data.\n\n#WFO.download()\n#WFO.remember(WFO.file = \"classification.csv\")\n\nRead in trnL data.\n\n#trnL_plants <- read_csv(\"../data/sequenced/trnL_reads_WeeTU.csv\")\n\nFind name issues in trnL data - this includes species with Authorship or other rows with strange names.\n\ntrnL_name_issues <- trnL_plants %>% \n   distinct(Species) %>% \n   mutate(Species = str_replace_all(Species, \"_\", \" \")) %>% \n   mutate(n_words = str_count(Species, \"\\\\w+\")) %>% \n   filter(n_words > 2)\n\n#write_csv(trnL_name_issues, \"../outputs/naming_issues/trnL_name_issues.csv\")\n\nReplace “_” with a space with gsub - see comment below.\n\n#not required to run but here for another example of how to replace characters\ntrnL_plants$Species <- gsub(\"_\",\" \", trnL_plants$Species)\n\nFix naming issues - see comment below. This code was kept as an example of how to update plant names. However this is an extremely long and time consuming way to mutate the data.\n\n#do not run - here just to keep - old code\ntrnL_plants <- trnL_plants %>% \n  mutate(Species = str_replace_all(Species, \"_\", \" \")) %>% \n  mutate(Species = replace(Species, Species == \"Chamaecrista sp  ASC-2009\", \"Chamaecrista sp.\"), \n         Species = replace(Species, Species == \"Setaria sp  TRK-2015\", \"Setaria sp.\"),\n         Species = replace(Species, Species == \"Sanango sp  Bremer 3352\", \"Sanango sp.\"),\n         Species = replace(Species, Species == \"Helichrysum sp  TRK-2015\", \"Helichrysum sp.\"),\n         Species = replace(Species, Species == \"Taraxacum sect  Naevosa sp  6281f\", \"Taraxacum (sect Naevosa) sp.\"),\n         Species = replace(Species, Species == \"Muehlenbeckia sp  Mt Brockman L A Craven 2357 K L Wilson & Makinson\", \"Muehlenbeckia sp.\"),\n         Species = replace(Species, Species == \"Sida sp  TRK-2015\", \"Sida sp.\"),\n         Species = replace(Species, Species == \"Eragrostis sp  TRK-2015\", \"Eragrostis sp.\"),\n         Species = replace(Species, Species == \"Dypsis sp  Mada25\", \"Dypsis sp.\"),\n         Species = replace(Species, Species == \"Quercus sp  MES114\", \"Quercus sp.\"),\n         Species = replace(Species, Species == \"Ichnanthus sp  Silva et al  550\", \"Ichnanthus sp.\"),\n         Species = replace(Species, Species == \"Juniperus hybrid sp  LO-2009\", \"Juniperus sp.\"),\n         Species = replace(Species, Species == \"Solanum sp  TRK-2015\", \"Solanum sp.\"),\n         Species = replace(Species, Species == \"Pavonia sp  TRK-2015\", \"Pavonia sp.\"),\n         Species = replace(Species, Species == \"Asteraceae sp  TRK-2015\", \"Asteraceae sp.\"),\n         Species = replace(Species, Species == \"Zygia sp  KGD-2009\", \"Zygia sp.\"),\n         Species = replace(Species, Species == \"Hildaea sp  Costa et al  903\", \"Hildaea sp.\"),\n         Species = replace(Species, Species == \"Gazania sp  Koekemoer and Funk 1929\", \"Gazania sp.\"),\n         Species = replace(Species, Species == \"Digitaria sp  TRK-2015\", \"Digitaria sp.\"),\n         Species = replace(Species, Species == \"Celtis sp  Mada221\", \"Celtis sp.\"),\n         Species = replace(Species, Species == \"Nicotiana sp  'rastroensis'\", \"Nicotiana sp.\"),\n         Species = replace(Species, Species == \"Castanopsis sp  'kuchugouzhui'\", \"Castanopsis sp.\"),\n         Species = replace(Species, Species == \"Asteroideae sp  D3-001\", \"Asteroideae sp.\"),\n         Species = replace(Species, Species == \"Operculina sp  Romero 1701\", \"Operculina sp.\"),\n         Species = replace(Species, Species == \"Musa sp  Ogasawara06\", \"Musa sp.\"),\n         Species = replace(Species, Species == \"Pithecellobium sp  DS14533 JM1598\", \"Pithecellobium sp.\"),\n         Species = replace(Species, Species == \"Excoecaria sp  Pell 678\", \"Excoecaria sp.\"),\n         Species = replace(Species, Species == \"Enneapogon sp  TRK-2015\", \"Enneapogon sp.\"),\n         Species = replace(Species, Species == \"Phyllarthron sp  Mada29\", \"Phyllarthron sp.\"),\n         Species = replace(Species, Species == \"Catalpa aff  speciosa Olmstead 88-003\", \"Catalpa sp.\"),\n         Species = replace(Species, Species == \"Piresia sp  Hodkinson 601\", \"Piresia sp.\"),\n         Species = replace(Species, Species == \"Calliandra sp  ERS-2013\", \"Calliandra sp.\"),\n         Species = replace(Species, Species == \"Sclerophylax sp  Nee and Bohs 50857\", \"Sclerophylax sp.\"))\n\nReplace names only identified to species level with an “NA” in the “Species” column.\n\ntrnL_plants <- trnL_plants %>% \n  mutate(Species = str_replace_all(Species, \"_\", \" \")) %>% \n  mutate(Species = replace(Species, Species == \"Chamaecrista sp  ASC-2009\", \"NA\"),\n         Species = replace(Species, Species == \"Setaria sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Sanango sp  Bremer 3352\", \"NA\"),\n         Species = replace(Species, Species == \"Helichrysum sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Taraxacum sect  Naevosa sp  6281f\", \"NA\"),\n         Species = replace(Species, Species == \"Muehlenbeckia sp  Mt Brockman L A Craven 2357 K L Wilson & Makinson\", \"NA\"),\n         Species = replace(Species, Species == \"Sida sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Eragrostis sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Dypsis sp  Mada25\", \"NA\"),\n         Species = replace(Species, Species == \"Quercus sp  MES114\", \"NA\"),\n         Species = replace(Species, Species == \"Ichnanthus sp  Silva et al  550\", \"NA\"),\n         Species = replace(Species, Species == \"Juniperus hybrid sp  LO-2009\", \"NA\"),\n         Species = replace(Species, Species == \"Solanum sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Pavonia sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Asteraceae sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Zygia sp  KGD-2009\", \"NA\"),\n         Species = replace(Species, Species == \"Hildaea sp  Costa et al  903\", \"NA\"),\n         Species = replace(Species, Species == \"Gazania sp  Koekemoer and Funk 1929\", \"NA\"),\n         Species = replace(Species, Species == \"Digitaria sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Celtis sp  Mada221\", \"NA\"),\n         Species = replace(Species, Species == \"Nicotiana sp  'rastroensis'\", \"NA\"),\n         Species = replace(Species, Species == \"Castanopsis sp  'kuchugouzhui'\", \"NA\"),\n         Species = replace(Species, Species == \"Asteroideae sp  D3-001\", \"NA\"),\n         Species = replace(Species, Species == \"Operculina sp  Romero 1701\", \"NA\"),\n         Species = replace(Species, Species == \"Musa sp  Ogasawara06\", \"NA\"),\n         Species = replace(Species, Species == \"Pithecellobium sp  DS14533 JM1598\", \"NA\"),\n         Species = replace(Species, Species == \"Excoecaria sp  Pell 678\", \"NA\"),\n         Species = replace(Species, Species == \"Enneapogon sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Phyllarthron sp  Mada29\", \"NA\"),\n         Species = replace(Species, Species == \"Catalpa aff  speciosa Olmstead 88-003\", \"NA\"),\n         Species = replace(Species, Species == \"Piresia sp  Hodkinson 601\", \"NA\"),\n         Species = replace(Species, Species == \"Calliandra sp  ERS-2013\", \"NA\"),\n         Species = replace(Species, Species == \"Sclerophylax sp  Nee and Bohs 50857\", \"NA\")) %>% \n  filter(Species != \"fossil Castanea pollen\")\n\nSelect only the distinct values in the “Species” column, and remove NAs/names that are not essential based on trnL_name_issues.csv file for WFO.match().\n\ntrnL_plant_species <- trnL_plants %>% \n  filter(Species != \"NA\") %>% \n  select(Species) %>% \n  distinct()\n\nConvert to data frame so WFO.match can run properly.\n\n# need to convert trnL_plant_species from a tibble to a dataframe\ntrnL_plant_species <- as.data.frame(trnL_plant_species)\n\nMatch trnL data with WFO data. WFO will match your data set with out of data names. Specifically it will add a new column of updated authorship, genus, etc.\n\ntrnL_plant_match <- WFO.match(spec.data = trnL_plant_species, \n                               WFO.data = WFO.data,\n                               spec.name = \"Species\", \n                               Authorship = '', \n                               First.dist = TRUE, \n                               Fuzzy.min = TRUE, \n                               Fuzzy = 0.1, \n                               Fuzzy.max = 250, \n                               Fuzzy.two = TRUE, \n                               Fuzzy.one = TRUE, \n                               squish = TRUE, \n                               spec.name.tolower = FALSE, \n                               spec.name.nonumber = TRUE, \n                               spec.name.nobrackets = TRUE, \n                               exclude.infraspecific = FALSE, \n                               verbose = TRUE, \n                               counter = 1000)\n\nWrite csv for plant matches and save to outputs folder. This file contains all plants that had a “match” with the WFO data. Otherwise, this means that the original plant name is outdated.\n\n#write_csv(trnL_plant_match, \"../outputs/trnL_plant_match.csv\")\n\nSelect plants that had fuzzy values and select needed columns from WFO.match data output.\n\nfuzzy_plants_trnL <- trnL_plant_match %>% \n  filter(Fuzzy == 'TRUE') %>% \n  select(Fuzzy,\n         Old.name,\n         family,\n         genus,\n         Species,\n         scientificName, \n         taxonID,\n         taxonomicStatus,\n         New.accepted)\n\nKeep only distinct updated scientific name matches, and remove subspecies distinction.\n\nfuzzy_plants_trnL <- fuzzy_plants_trnL %>% \n  mutate(scientificName = replace(scientificName, scientificName == \"Diplotaxis erucoides  subsp. cossoniana\", \"Diplotaxis erucoides\")) %>% \n  distinct(scientificName, .keep_all = TRUE)\n\nAdd updated names to ITS2 data with a left join.\n\nupdated_trnL <- trnL_plants %>% \n  left_join(fuzzy_plants_trnL, by = \"Species\") \n\nRename columns and deselect unneeded columns.\n\nupdated_trnL <- updated_trnL %>% \n  rename(updated.Name = scientificName) %>% \n  rename(updated.genus = genus) %>% \n  rename(updated.family = family) %>% \n  select(-c(Fuzzy, Old.name, taxonomicStatus)) %>% \n  distinct() \n\nReplace old names, genus, and family with new names in trnL file using ifelse statement.\n\nfinal_updated_trnL <- updated_trnL %>% \n  mutate(Species = if_else(!is.na(updated.Name),\n                           true = updated.Name,\n                           false = Species)) %>% \n  mutate(Genus = if_else(!is.na(updated.genus),\n                           true = updated.genus,\n                           false = Genus)) %>% \n  mutate(Family = if_else(!is.na(updated.family),\n                          true = updated.family,\n                          false = Family))\n\nQuality check to make sure it worked!\n\nfinal_updated_trnL %>% \n  filter(Species == \"Reicheocactus famatinensis\")\n\nfinal_updated_trnL %>% \n  filter(Species == \"Lycium bridgesii\")\n\nRecreate WTUs. This is a specific identifier with a set code format used for this dataset.\n\n# Add WeeTUs for Summarizing\n# https://dplyr.tidyverse.org/reference/group_data.html\nupdated_trnL_reads_WeeTU <- final_updated_trnL %>% \n  mutate(WTU.kingdom = group_indices(., Kingdom),\n         WTU.clade1 = group_indices(., Kingdom, Clade1),\n         WTU.clade2 = group_indices(., Kingdom, Clade1, Clade2),\n         WTU.order = group_indices(., Kingdom, Clade1, Clade2, Order),\n         WTU.family = group_indices(., Kingdom, Clade1, Clade2, Order, Family),\n         WTU.subfamily = group_indices(., Kingdom, Clade1, Clade2, Order, \n                                       Family, Subfamily),\n         WTU.genus = group_indices(., Kingdom, Clade1, Clade2, Order, Family, \n                                   Subfamily, Genus),\n         WTU.species = group_indices(., Kingdom, Clade1, Clade2, Order, Family, \n                                     Subfamily, Genus, Species))\n\nRemove unneeded columns and save final data frame as a csv to outputs folder.\n\nupdated_trnL_reads_WeeTU <- updated_trnL_reads_WeeTU %>% \n  select(-c(updated.family, updated.genus, updated.Name, New.accepted))\n\n#write_csv(final_updated_trnL, \"../outputs/updated_trnL_reads_WeeTU.csv\")"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Projects",
    "section": "",
    "text": "Python Data Wrangling Workshop\n\n\n\n\n\n\n\nPython\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nBailie Wynbelt\n\n\n\n\n\n\n  \n\n\n\n\nETL: Extract, Transform, Load\n\n\n\n\n\n\n\nPython\n\n\nSQL\n\n\nAWS\n\n\nValidation\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nBailie Wynbelt\n\n\n\n\n\n\n  \n\n\n\n\nData Wrangling, Updates, and Validation\n\n\n\n\n\n\n\nR\n\n\nWrangling\n\n\nValidation\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nBailie Wynbelt\n\n\n\n\n\n\n  \n\n\n\n\nR - Palmer Penguin Analysis and Visualization\n\n\n\n\n\n\n\nR\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nBailie Wynbelt\n\n\n\n\n\n\nNo matching items"
  }
]