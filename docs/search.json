[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello, I’m Bailie! I’m currently pursuing a Bachelor of Science in Information Science with a focus in Data Science at the University of Arizona. I am deeply interested in the intersection of ecology and data science.\n\nMy journey into data science began while I was studying Natural Resources. I took a class focused on statistics and coding with an emphasis on ecology. I fell in love with the idea of combining data science and ecology, leading me to make the switch to my current degree.\n\nMy overall mission is to tackle challenging real-world problems by harnessing the power of data. My main skills fall in the realm of data wrangling, manipulation, analysis, visualization, and management in addition to front-end development with the overall goal of driving decision-making.\n\nIn addition to my academic and professional endeavors, I strive for diversity and inclusion in the tech community, with a particular interest in the queer tech community. I believe in the importance of fostering an inclusive environment where everyone’s talents and perspectives are valued!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bailie Wynbelt",
    "section": "",
    "text": "Hello! My name is Bailie.\nI am currently pursuing a Bachelor of Science in Information Science with an emphasis in Data Science. My expected graduation data is May 2024.\nIn my spare time, you can find me hiking around beautiful southern Arizona or crocheting!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Bailie Wynbelt",
    "section": "Education",
    "text": "Education\nUniversity of Arizona | Tucson, AZ | B.S in Information Science | August 2020 - May 2024"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Bailie Wynbelt",
    "section": "Experience",
    "text": "Experience\nNational Phenology Network | Systems Assistant | October 2023 - present\nVirga Labs | Data Science Intern | May 2023 - September 2023\nSchool of Natural Resources and the Environment | Data Science Research Assistant | January 2023 - present\nSchool of Natural Resources and the Environment ~ Ruyle Lab | Data Science Intern | August 2022 - December 2022"
  },
  {
    "objectID": "posts/etl/index.html",
    "href": "posts/etl/index.html",
    "title": "ETL: Extract, Transform, Load",
    "section": "",
    "text": "What can I say… I LOVE THE ETL PROCESS. It is so fun and groovy. This document contains code that was written for my data engineering class. Some of the code was provided by the professor - however it is noted when the code was not written by me.\nThe goal of this homework was to take a semi-structured non-normalized CSV file and turn it into a set of normalized tables that was then pushed to a postgres database hosted on AWS. Throughout the homework, I created a database schema and created tables with primary and foreign keys within the database. The database contains tables cases, judges, and casetype. These tables contain information about judge name, political party, gender, race, case year, casetype, etc.\nThis data could be used for a wide range of uses. For example, analysts could use the final summary table (that is created at the end of this document) to look at how political party inpacts court decisions."
  },
  {
    "objectID": "posts/etl/index.html#bring-in-data-and-explore.",
    "href": "posts/etl/index.html#bring-in-data-and-explore.",
    "title": "ETL: Extract, Transform, Load",
    "section": "Bring in data and explore.",
    "text": "Bring in data and explore.\nThe goal of this section is to import the data and gain a general overview of the data.\nWe cannot forget our import statements!\n\nimport pandas as pd\nimport psycopg2\nimport numpy as np\nfrom pandas._libs.lib import count_level_2d\n\nFirst, we need to read in the data to get started.\n\n#written by professor\ncases = pd.read_csv('https://docs.google.com/spreadsheets/d/1AWLK06JOlSKImgoHNTbj7oXR5mRfsL2WWeQF6ofMq1g/gviz/tq?tqx=out:csv')\n\nLets look at the first and last 5 rows of the data.\n\nprint(cases.head())\nprint(cases.tail())\n\nNow we know what columns our dataframe contains…but what are the data types of each? Are they what we could expect?\n\ncases.dtypes"
  },
  {
    "objectID": "posts/etl/index.html#create-functions-that-will-help-execute-our-queries-and-checksvalidation-faster.",
    "href": "posts/etl/index.html#create-functions-that-will-help-execute-our-queries-and-checksvalidation-faster.",
    "title": "ETL: Extract, Transform, Load",
    "section": "Create functions that will help execute our queries and checks/validation faster.",
    "text": "Create functions that will help execute our queries and checks/validation faster.\nThe following code was written by the professor - however it is crucial to the checks and execution of the database creation.\n\n# run_query function\ndef run_query(query_string):\n\n conn, cur = get_conn_cur() # get connection and cursor\n\n cur.execute(query_string) # executing string as before\n\n my_data = cur.fetchall() # fetch query data as before\n\n # here we're extracting the 0th element for each item in cur.description\n colnames = [desc[0] for desc in cur.description]\n\n cur.close() # close\n conn.close() # close\n\n return(colnames, my_data) # return column names AND data\n\n# Column name function for checking out what's in a table\ndef get_column_names(table_name): # arguement of table_name\n conn, cur = get_conn_cur() # get connection and cursor\n\n # Now select column names while inserting the table name into the WERE\n column_name_query = \"\"\"SELECT column_name FROM information_schema.columns\n    WHERE table_name = '%s' \"\"\" %table_name\n\n cur.execute(column_name_query) # exectue\n my_data = cur.fetchall() # store\n\n cur.close() # close\n conn.close() # close\n\n return(my_data) # return\n\n# Check table_names\ndef get_table_names():\n  conn, cur = get_conn_cur() # get connection and cursor\n\n  # query to get table names\n  table_name_query = \"\"\"SELECT table_name FROM information_schema.tables\n       WHERE table_schema = 'public' \"\"\"\n\n  cur.execute(table_name_query) # execute\n  my_data = cur.fetchall() # fetch results\n\n  cur.close() #close cursor\n  conn.close() # close connection\n\n  return(my_data) # return your fetched results\n\n# make sql_head function\ndef sql_head(table_name):\n conn, cur = get_conn_cur() # get connection and cursor\n\n # Now select column names while inserting the table name into the WERE\n head_query = \"\"\"SELECT * FROM %s LIMIT 5; \"\"\" %table_name\n\n cur.execute(head_query) # exectue\n colnames = [desc[0] for desc in cur.description] # get column names\n my_data = cur.fetchall() # store first five rows\n\n cur.close() # close\n conn.close() # close\n\n df = pd.DataFrame(data = my_data, columns = colnames) # make into df\n\n return(df) # return\n\ndef my_drop_table(tab_name):\n  conn, cur = get_conn_cur()\n  tq = \"\"\"DROP TABLE IF EXISTS %s CASCADE;\"\"\" %tab_name\n  cur.execute(tq)\n  conn.commit()"
  },
  {
    "objectID": "posts/etl/index.html#make-cases-table-and-push-to-database.",
    "href": "posts/etl/index.html#make-cases-table-and-push-to-database.",
    "title": "ETL: Extract, Transform, Load",
    "section": "Make cases table and push to database.",
    "text": "Make cases table and push to database.\nFor this section I will be creating the first table of the schema: cases. This table will contain the columns case_id, case_year, category_name, casetype_id, judge_id, libcon_name. The data will be linked to the judge table by the judge_id column.\nFirst we need to make a foreign key to connect with the judge table.\n\n# Make judge_id in cases\ncases['judge_id'] = pd.factorize(cases['judge_name'])[0].astype(str)\ncases\n\nSelect necessary columns to make cases_df.\n\ncases_df = cases[['case_id','case_year', 'category_name', 'casetype_id', 'judge_id', 'libcon_name']]\ncases_df\n\nThis is creating a database connection - confidential information such as user and password are not included for safety:)\n\n#written by professor\ndef get_conn_cur(): # define function name and arguments (there aren't any)\n conn = psycopg2.connect(\n    host=\"\",\n    database=\"\",\n    user=\"\",\n    password=\"\",\n    port='5432'\n    )\n cur = conn.cursor()   # Make a cursor after\n return(conn, cur)   # Return both the connection and the cursor\n\nMake table in database.\n\n#make table\ntq = \"\"\"CREATE TABLE cases (\n     case_id INT PRIMARY KEY,\n     case_year INT NOT NULL ,\n     category_name VARCHAR(255) NOT NULL,\n     casetype_id INT NOT NULL,\n     judge_id INT NOT NULL,\n     libcon_name VARCHAR(255) NOT NULL\n     );\"\"\"\n\nExecute and commit table!\n\nconn, cur = get_conn_cur()\ncur.execute(tq)\nconn.commit()\n\nCheck column names to make sure everything looks correct.\n\nget_column_names(table_name='cases')\n\nConvert into tuple.\n\ncases_tuple = cases_df.to_numpy();\ncases_tuple[:,1] = np.vectorize(lambda x: str(x))(cases_tuple[:,1])\ndata_tups = [tuple(x) for x in cases_tuple]\n\nwrite SQL string to add local data to database.\n\niq = \"\"\"INSERT INTO cases(case_id, case_year, category_name, casetype_id, judge_id, libcon_name) VALUES(%s, %s, %s, %s, %s, %s);\"\"\"\n\nCommit and check complete table.\n\n#Execute the string\nconn, cur = get_conn_cur()\ncur.executemany(iq, data_tups)\n\n#Commit and check\nconn.commit()\nconn.close()\n\n#check\nsql_head(table_name='cases')"
  },
  {
    "objectID": "posts/etl/index.html#make-a-rollup-table",
    "href": "posts/etl/index.html#make-a-rollup-table",
    "title": "ETL: Extract, Transform, Load",
    "section": "Make a rollup table",
    "text": "Make a rollup table\nThe process used to create the cases table is the same process that was used to create the other connecting tables (judges and casetype). To avoid repition, I will be skipping this code. However, I will be using the remaining code that was written. This has the goal of creating a summary table analysts could use to look at case decisions and political party type.\nFirst, I made a groupby called cases_rollup. This groups by party_name and category name and aggregates the count and sum of libcon_id.\n\ncases_rollup = cases.groupby(['category_name', 'party_name'])['libcon_id'].agg(['sum', 'count']).reset_index()\ncases_rollup\n\ncases_rollup.reset_index()\n\n#Rename columns to be consistant with new values.\n\n# rename your columns now. Keep the first to the same but call the last two 'total_cases' and 'num_lib_decisions'\ncases_rollup.rename(columns={'sum': 'num_lib_decisions', 'count': 'total_cases'}, inplace = True)\ncases_rollup\n\nNow, I created a new column called ‘percent_liberal’. This calculates the percentage of decisions that were liberal in nature.\n\ncases_rollup['percent_liberal'] = round((cases_rollup['num_lib_decisions'] / cases_rollup['total_cases']) * 100)\ncases_rollup"
  },
  {
    "objectID": "posts/iris_python/index.html",
    "href": "posts/iris_python/index.html",
    "title": "Python - Iris Analysis and Visualization",
    "section": "",
    "text": "I used the free Iris data set and python to conduct analysis and create visualizations"
  },
  {
    "objectID": "posts/iris_python/index.html#importing-and-loading-required-packages",
    "href": "posts/iris_python/index.html#importing-and-loading-required-packages",
    "title": "Python - Iris Analysis and Visualization",
    "section": "Importing and loading required packages",
    "text": "Importing and loading required packages\nThe following code is importing both the pandas and matplotlib.pyplot package so specific functions can be used to analyze and visualize the data! Lets get started with exploring. It is important to note that these packages need to be installed first on your computer before you can run this\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/iris_python/index.html#importing-iris-data-set",
    "href": "posts/iris_python/index.html#importing-iris-data-set",
    "title": "Python - Iris Analysis and Visualization",
    "section": "Importing iris data set",
    "text": "Importing iris data set\nThe iris data set is a free data set that is available! Below I am importing the data using the pd.read_csv function.\n\n# Create column names for dataframe\ncol_names = ['sepal_length_cm','sepal_width_cm','petal_length_cm','petal_width_cm','species']\n\n# Download data from url\ncsv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n\n# Import url using read_csv\niris = pd.read_csv(csv_url, names = col_names)"
  },
  {
    "objectID": "posts/iris_python/index.html#gaining-a-general-oversight-of-the-data",
    "href": "posts/iris_python/index.html#gaining-a-general-oversight-of-the-data",
    "title": "Python - Iris Analysis and Visualization",
    "section": "Gaining a general oversight of the data",
    "text": "Gaining a general oversight of the data\nThe head() and tail() functions give a general oversight of what the data is comprised of in terms of rows and columns. Moreover, we can view the first and last 5 rows of data.\n\niris.head()\niris.tail()\n\n\n\n\n\n  \n    \n      \n      sepal_length_cm\n      sepal_width_cm\n      petal_length_cm\n      petal_width_cm\n      species\n    \n  \n  \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      Iris-virginica\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      Iris-virginica\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      Iris-virginica\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      Iris-virginica\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      Iris-virginica\n    \n  \n\n\n\n\nGreat! Now we know what our data looks like. However, I want to make sure the data format is okay. To do so, I am going to confirm the data types of iris using the .dtypes. function. We can now see that the columns are all floats except the species column. This looks all good!\n\niris.dtypes\n\nsepal_length_cm    float64\nsepal_width_cm     float64\npetal_length_cm    float64\npetal_width_cm     float64\nspecies             object\ndtype: object"
  },
  {
    "objectID": "posts/iris_python/index.html#data-tidying",
    "href": "posts/iris_python/index.html#data-tidying",
    "title": "Python - Iris Analysis and Visualization",
    "section": "Data tidying",
    "text": "Data tidying\nThe following lines of code are tidying the data set. The code is looking to see if the data set have any NA’s. We can now see that there is zero NA’s. This means we do not need to drop any rows.\n\niris.isna().sum().sum()\n\n0"
  },
  {
    "objectID": "posts/iris_python/index.html#data-manipulation",
    "href": "posts/iris_python/index.html#data-manipulation",
    "title": "Python - Iris Analysis and Visualization",
    "section": "Data manipulation",
    "text": "Data manipulation\nThe following lines of code are manipulation the data set. The code is converting centimeters to millimeters. First, I created a function that converts from cm to mm and used .apply to apply to specific columns.\n\ndef cm_to_mm(cm):\n    return cm * 10\n\niris['sepal_length_mm'] = iris['sepal_length_cm'].apply(cm_to_mm)\niris['sepal_length_mm'] = iris['sepal_width_cm'].apply(cm_to_mm)\niris['petal_length_mm'] = iris['petal_length_cm'].apply(cm_to_mm)\n\niris\n\n\n\n\n\n  \n    \n      \n      sepal_length_cm\n      sepal_width_cm\n      petal_length_cm\n      petal_width_cm\n      species\n      sepal_length_mm\n      petal_length_mm\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      Iris-setosa\n      35.0\n      14.0\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      Iris-setosa\n      30.0\n      14.0\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      Iris-setosa\n      32.0\n      13.0\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      Iris-setosa\n      31.0\n      15.0\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      Iris-setosa\n      36.0\n      14.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      145\n      6.7\n      3.0\n      5.2\n      2.3\n      Iris-virginica\n      30.0\n      52.0\n    \n    \n      146\n      6.3\n      2.5\n      5.0\n      1.9\n      Iris-virginica\n      25.0\n      50.0\n    \n    \n      147\n      6.5\n      3.0\n      5.2\n      2.0\n      Iris-virginica\n      30.0\n      52.0\n    \n    \n      148\n      6.2\n      3.4\n      5.4\n      2.3\n      Iris-virginica\n      34.0\n      54.0\n    \n    \n      149\n      5.9\n      3.0\n      5.1\n      1.8\n      Iris-virginica\n      30.0\n      51.0\n    \n  \n\n150 rows × 7 columns"
  },
  {
    "objectID": "posts/iris_python/index.html#data-analysis-of-species",
    "href": "posts/iris_python/index.html#data-analysis-of-species",
    "title": "Python - Iris Analysis and Visualization",
    "section": "Data analysis of species",
    "text": "Data analysis of species\nBelow are lines of code that are summarizing the data based on specific attributes. The following is grouping the data by flower species, and calculating the mean of sepal length, sepal width, and petal length. This data is added to a new data frame called “species_summary”. For the purpose of this question, we are only interested in using columns that contain data in cm.\n\nspecies_summary = iris.groupby('species').agg({\n    'sepal_length_cm': ['mean', 'min', 'max'],\n    'sepal_width_cm': ['mean', 'min', 'max'],\n    'petal_length_cm': ['mean', 'min', 'max']}).reset_index()\n\n\nspecies_summary.columns = ['species', \n'mean_sepal_length', 'min_sepal_length','max_sepal_length',\n'mean_sepal_width', 'min_sepal_width', 'max_sepal_width',\n'mean_petal_length', 'min_petal_length', 'max_petal_length']"
  },
  {
    "objectID": "posts/iris_python/index.html#data-visualization-of-sepal-length-vs-sepal-width-of-species",
    "href": "posts/iris_python/index.html#data-visualization-of-sepal-length-vs-sepal-width-of-species",
    "title": "Python - Iris Analysis and Visualization",
    "section": "Data visualization of sepal length vs sepal width of species",
    "text": "Data visualization of sepal length vs sepal width of species\nNow, lets visualize the data! This will help us visualize if there are any notable differences in sepal length vs width of different Iris species. The code lines below are utilizing the matlibplot package to visualize the data. This data is a scatter plot with sepal length on the x-axis and sepal width on the y-axis, with the color of data points being differentiated by species Additionally, the graph is faceted by species of the penguins\n\n# Create colorblind friendly palette\nc_palette = [\"#377eb8\", \"#ff7f00\", \"#4daf4a\"]\n\n# Create plot\niris_plot = sns.FacetGrid(iris, hue = \"species\", palette = c_palette)\niris_plot.map(plt.scatter, \"sepal_length_cm\", \"sepal_width_cm\")\n\n# Add regression line\niris_plot.map(sns.regplot, \"sepal_length_cm\", \"sepal_width_cm\", scatter = False)\n\n# Add legend\niris_plot.add_legend(title=\"Iris Species:\", frameon = True, facecolor = \"lightgrey\")\n\n# Fix labels\niris_plot.set_axis_labels(\"Sepal Length (cm)\", \"Sepal Width (cm)\")\niris_plot.set_titles(\"Scatterplot with Linear Regression Lines by Species\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/palmer_R/index.html",
    "href": "posts/palmer_R/index.html",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "",
    "text": "I used the free Palmer Penguins data set in R to conduct analysis and create visualizations"
  },
  {
    "objectID": "posts/palmer_R/index.html#reading-tidyverse",
    "href": "posts/palmer_R/index.html#reading-tidyverse",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Reading tidyverse",
    "text": "Reading tidyverse\nThe following lines of code is reading in the tidyverse package so specific functions can be used to analyze the data.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "posts/palmer_R/index.html#loading-in-the-palmer-penguin-data-set",
    "href": "posts/palmer_R/index.html#loading-in-the-palmer-penguin-data-set",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Loading in the palmer penguin data set",
    "text": "Loading in the palmer penguin data set\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.2.3\n\npenguin_data <- penguins"
  },
  {
    "objectID": "posts/palmer_R/index.html#gaining-a-general-oversight-of-the-data",
    "href": "posts/palmer_R/index.html#gaining-a-general-oversight-of-the-data",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Gaining a general oversight of the data",
    "text": "Gaining a general oversight of the data\nThe head() and tail() functions give a general oversight of what the data is comprised of in terms of rows and columns.\n\nhead(penguin_data)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\ntail(penguin_data)\n\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Chinstrap Dream            45.7          17            195    3650 fema…  2009\n2 Chinstrap Dream            55.8          19.8          207    4000 male   2009\n3 Chinstrap Dream            43.5          18.1          202    3400 fema…  2009\n4 Chinstrap Dream            49.6          18.2          193    3775 male   2009\n5 Chinstrap Dream            50.8          19            210    4100 male   2009\n6 Chinstrap Dream            50.2          18.7          198    3775 fema…  2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-tidying",
    "href": "posts/palmer_R/index.html#data-tidying",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data tidying",
    "text": "Data tidying\nThe following lines of code are tidying the “penguin_data” data set. The code is removing any values in the data set that have an NA and creating a new data set called “clean_penguin_data”\n\nclean_penguin_data <- penguin_data %>%\n  drop_na()"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-manipulation",
    "href": "posts/palmer_R/index.html#data-manipulation",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data manipulation",
    "text": "Data manipulation\nThe following lines of code are manipulation the “clean_penguin_data” data set. The code is converting millimeters to centimeters, and grams to milligrams. New columns will be created in the clean_penguin_data\n\nclean_penguin_data <- clean_penguin_data %>%\n  mutate(bill_length_cm = bill_length_mm/10,\n         bill_depth_cm = bill_depth_mm/10,\n         flipper_length_cm = flipper_length_mm/10,\n         body_mass_mg = body_mass_g/1000)"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-analysis-of-islands",
    "href": "posts/palmer_R/index.html#data-analysis-of-islands",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data analysis of islands",
    "text": "Data analysis of islands\nBelow are lines of code that are summarizing the “clean_penguin_data” based on specific attributes. The following is grouping the data by island, and calculating the mean of bill length, bill depth, flipper length, and body mass. This data is added to a new data frame called “island_penguin_summary”.\n\nisland_penguin_summary <- clean_penguin_data %>%\n  group_by(island) %>%\n  summarize(mean_bill_length_mm = mean(bill_length_mm),\n            mean_bill_depth_mm = mean(bill_depth_mm),\n            mean_flipper_length_mm = mean(flipper_length_mm),\n            mean_body_mass_g = mean(body_mass_g))"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-analysis-of-sex",
    "href": "posts/palmer_R/index.html#data-analysis-of-sex",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data analysis of sex",
    "text": "Data analysis of sex\nBelow are lines of code that are summarizing the “clean_penguin_data” based on specific attributes. The following is grouping the data by sex, and calculating the mean of bill length, bill depth, flipper length, and body mass. This data is added to a new data frame called “sex_penguin_summary”.\n\nsex_penguin_summary <- clean_penguin_data %>%\n  group_by(sex) %>%\n  summarize(mean_bill_length_mm = mean(bill_length_mm),\n            mean_bill_depth_mm = mean(bill_depth_mm),\n            mean_flipper_length_mm = mean(flipper_length_mm),\n            mean_body_mass_g = mean(body_mass_g))"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-analysis-of-species",
    "href": "posts/palmer_R/index.html#data-analysis-of-species",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data analysis of species",
    "text": "Data analysis of species\nBelow are lines of code that are summarizing the “clean_penguin_data” based on specific attributes. The following is grouping the data by species, and calculating the mean of bill length, bill depth, flipper length, and body mass. This data is added to a new data frame called “species_penguin_summary”.\n\nspecies_penguin_summary <- clean_penguin_data %>%\n  group_by(species) %>%\n  summarize(bill_length_mm = mean(bill_length_mm),\n            mean_bill_depth_mm = mean(bill_depth_mm),\n            mean_flipper_length_mm = mean(flipper_length_mm),\n            mean_body_mass_g = mean(body_mass_g))"
  },
  {
    "objectID": "posts/palmer_R/index.html#data-visualization-of-body-mass-vs-flipper-length-of-penguins",
    "href": "posts/palmer_R/index.html#data-visualization-of-body-mass-vs-flipper-length-of-penguins",
    "title": "R - Palmer Penguin Analysis and Visualization",
    "section": "Data visualization of body mass vs flipper length of penguins",
    "text": "Data visualization of body mass vs flipper length of penguins\nThe code lines below are utilizing the ggplot package to visualize the “clean_penguin_data”. This data is a scatter plot with body mass on the x-axis and flipper length on the y-axis, with the color of data points being differentiated by color. Additionally, the graph is facted by species of the penguins\n\nggplot(data = clean_penguin_data,\n       mapping = aes(x = body_mass_g, \n                     y = flipper_length_mm,\n                     color = sex)) +\n  geom_point() +\n  facet_wrap(~species) +\n  labs(x = \"Body Mass of Penguins (g)\",\n       y = \"Flipper Length of Penguins (mm)\",\n       title = \"Body Mass (g) VS Flipper Length (mm) of Penguins\") +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/wrangling_R/index.html",
    "href": "posts/wrangling_R/index.html",
    "title": "Data Wrangling, Updates, and Validation",
    "section": "",
    "text": "The following code is a representation of my work in School of Natural Resources. I was using data that was collected in 2017 and we wanted to run new analyses.\nThe data consists of rodents samples that were collected in the southwest. The goal of this data was to display what rodents are eating and how they are diet partitioning.\nThis document shows the wrangling, update, and validation of the large data using the package Worldflora to find updated taxonomic names.\nIt is important to note that the data will not be displayed in this document. The data is large and confidental. However, my code is still in the useable format and results in a dataset with updated taxonomic names and codes!\nInstall and load packages.\n\nlibrary(WorldFlora)\nlibrary(tidyverse)\n\nDownload and remember WFO data.\n\n#WFO.download()\n#WFO.remember(WFO.file = \"classification.csv\")\n\nRead in trnL data.\n\n#trnL_plants <- read_csv(\"../data/sequenced/trnL_reads_WeeTU.csv\")\n\nFind name issues in trnL data - this includes species with Authorship or other rows with strange names.\n\ntrnL_name_issues <- trnL_plants %>% \n   distinct(Species) %>% \n   mutate(Species = str_replace_all(Species, \"_\", \" \")) %>% \n   mutate(n_words = str_count(Species, \"\\\\w+\")) %>% \n   filter(n_words > 2)\n\n#write_csv(trnL_name_issues, \"../outputs/naming_issues/trnL_name_issues.csv\")\n\nReplace “_” with a space with gsub - see comment below.\n\n#not required to run but here for another example of how to replace characters\ntrnL_plants$Species <- gsub(\"_\",\" \", trnL_plants$Species)\n\nFix naming issues - see comment below. This code was kept as an example of how to update plant names. However this is an extremely long and time consuming way to mutate the data.\n\n#do not run - here just to keep - old code\ntrnL_plants <- trnL_plants %>% \n  mutate(Species = str_replace_all(Species, \"_\", \" \")) %>% \n  mutate(Species = replace(Species, Species == \"Chamaecrista sp  ASC-2009\", \"Chamaecrista sp.\"), \n         Species = replace(Species, Species == \"Setaria sp  TRK-2015\", \"Setaria sp.\"),\n         Species = replace(Species, Species == \"Sanango sp  Bremer 3352\", \"Sanango sp.\"),\n         Species = replace(Species, Species == \"Helichrysum sp  TRK-2015\", \"Helichrysum sp.\"),\n         Species = replace(Species, Species == \"Taraxacum sect  Naevosa sp  6281f\", \"Taraxacum (sect Naevosa) sp.\"),\n         Species = replace(Species, Species == \"Muehlenbeckia sp  Mt Brockman L A Craven 2357 K L Wilson & Makinson\", \"Muehlenbeckia sp.\"),\n         Species = replace(Species, Species == \"Sida sp  TRK-2015\", \"Sida sp.\"),\n         Species = replace(Species, Species == \"Eragrostis sp  TRK-2015\", \"Eragrostis sp.\"),\n         Species = replace(Species, Species == \"Dypsis sp  Mada25\", \"Dypsis sp.\"),\n         Species = replace(Species, Species == \"Quercus sp  MES114\", \"Quercus sp.\"),\n         Species = replace(Species, Species == \"Ichnanthus sp  Silva et al  550\", \"Ichnanthus sp.\"),\n         Species = replace(Species, Species == \"Juniperus hybrid sp  LO-2009\", \"Juniperus sp.\"),\n         Species = replace(Species, Species == \"Solanum sp  TRK-2015\", \"Solanum sp.\"),\n         Species = replace(Species, Species == \"Pavonia sp  TRK-2015\", \"Pavonia sp.\"),\n         Species = replace(Species, Species == \"Asteraceae sp  TRK-2015\", \"Asteraceae sp.\"),\n         Species = replace(Species, Species == \"Zygia sp  KGD-2009\", \"Zygia sp.\"),\n         Species = replace(Species, Species == \"Hildaea sp  Costa et al  903\", \"Hildaea sp.\"),\n         Species = replace(Species, Species == \"Gazania sp  Koekemoer and Funk 1929\", \"Gazania sp.\"),\n         Species = replace(Species, Species == \"Digitaria sp  TRK-2015\", \"Digitaria sp.\"),\n         Species = replace(Species, Species == \"Celtis sp  Mada221\", \"Celtis sp.\"),\n         Species = replace(Species, Species == \"Nicotiana sp  'rastroensis'\", \"Nicotiana sp.\"),\n         Species = replace(Species, Species == \"Castanopsis sp  'kuchugouzhui'\", \"Castanopsis sp.\"),\n         Species = replace(Species, Species == \"Asteroideae sp  D3-001\", \"Asteroideae sp.\"),\n         Species = replace(Species, Species == \"Operculina sp  Romero 1701\", \"Operculina sp.\"),\n         Species = replace(Species, Species == \"Musa sp  Ogasawara06\", \"Musa sp.\"),\n         Species = replace(Species, Species == \"Pithecellobium sp  DS14533 JM1598\", \"Pithecellobium sp.\"),\n         Species = replace(Species, Species == \"Excoecaria sp  Pell 678\", \"Excoecaria sp.\"),\n         Species = replace(Species, Species == \"Enneapogon sp  TRK-2015\", \"Enneapogon sp.\"),\n         Species = replace(Species, Species == \"Phyllarthron sp  Mada29\", \"Phyllarthron sp.\"),\n         Species = replace(Species, Species == \"Catalpa aff  speciosa Olmstead 88-003\", \"Catalpa sp.\"),\n         Species = replace(Species, Species == \"Piresia sp  Hodkinson 601\", \"Piresia sp.\"),\n         Species = replace(Species, Species == \"Calliandra sp  ERS-2013\", \"Calliandra sp.\"),\n         Species = replace(Species, Species == \"Sclerophylax sp  Nee and Bohs 50857\", \"Sclerophylax sp.\"))\n\nReplace names only identified to species level with an “NA” in the “Species” column.\n\ntrnL_plants <- trnL_plants %>% \n  mutate(Species = str_replace_all(Species, \"_\", \" \")) %>% \n  mutate(Species = replace(Species, Species == \"Chamaecrista sp  ASC-2009\", \"NA\"),\n         Species = replace(Species, Species == \"Setaria sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Sanango sp  Bremer 3352\", \"NA\"),\n         Species = replace(Species, Species == \"Helichrysum sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Taraxacum sect  Naevosa sp  6281f\", \"NA\"),\n         Species = replace(Species, Species == \"Muehlenbeckia sp  Mt Brockman L A Craven 2357 K L Wilson & Makinson\", \"NA\"),\n         Species = replace(Species, Species == \"Sida sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Eragrostis sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Dypsis sp  Mada25\", \"NA\"),\n         Species = replace(Species, Species == \"Quercus sp  MES114\", \"NA\"),\n         Species = replace(Species, Species == \"Ichnanthus sp  Silva et al  550\", \"NA\"),\n         Species = replace(Species, Species == \"Juniperus hybrid sp  LO-2009\", \"NA\"),\n         Species = replace(Species, Species == \"Solanum sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Pavonia sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Asteraceae sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Zygia sp  KGD-2009\", \"NA\"),\n         Species = replace(Species, Species == \"Hildaea sp  Costa et al  903\", \"NA\"),\n         Species = replace(Species, Species == \"Gazania sp  Koekemoer and Funk 1929\", \"NA\"),\n         Species = replace(Species, Species == \"Digitaria sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Celtis sp  Mada221\", \"NA\"),\n         Species = replace(Species, Species == \"Nicotiana sp  'rastroensis'\", \"NA\"),\n         Species = replace(Species, Species == \"Castanopsis sp  'kuchugouzhui'\", \"NA\"),\n         Species = replace(Species, Species == \"Asteroideae sp  D3-001\", \"NA\"),\n         Species = replace(Species, Species == \"Operculina sp  Romero 1701\", \"NA\"),\n         Species = replace(Species, Species == \"Musa sp  Ogasawara06\", \"NA\"),\n         Species = replace(Species, Species == \"Pithecellobium sp  DS14533 JM1598\", \"NA\"),\n         Species = replace(Species, Species == \"Excoecaria sp  Pell 678\", \"NA\"),\n         Species = replace(Species, Species == \"Enneapogon sp  TRK-2015\", \"NA\"),\n         Species = replace(Species, Species == \"Phyllarthron sp  Mada29\", \"NA\"),\n         Species = replace(Species, Species == \"Catalpa aff  speciosa Olmstead 88-003\", \"NA\"),\n         Species = replace(Species, Species == \"Piresia sp  Hodkinson 601\", \"NA\"),\n         Species = replace(Species, Species == \"Calliandra sp  ERS-2013\", \"NA\"),\n         Species = replace(Species, Species == \"Sclerophylax sp  Nee and Bohs 50857\", \"NA\")) %>% \n  filter(Species != \"fossil Castanea pollen\")\n\nSelect only the distinct values in the “Species” column, and remove NAs/names that are not essential based on trnL_name_issues.csv file for WFO.match().\n\ntrnL_plant_species <- trnL_plants %>% \n  filter(Species != \"NA\") %>% \n  select(Species) %>% \n  distinct()\n\nConvert to data frame so WFO.match can run properly.\n\n# need to convert trnL_plant_species from a tibble to a dataframe\ntrnL_plant_species <- as.data.frame(trnL_plant_species)\n\nMatch trnL data with WFO data. WFO will match your data set with out of data names. Specifically it will add a new column of updated authorship, genus, etc.\n\ntrnL_plant_match <- WFO.match(spec.data = trnL_plant_species, \n                               WFO.data = WFO.data,\n                               spec.name = \"Species\", \n                               Authorship = '', \n                               First.dist = TRUE, \n                               Fuzzy.min = TRUE, \n                               Fuzzy = 0.1, \n                               Fuzzy.max = 250, \n                               Fuzzy.two = TRUE, \n                               Fuzzy.one = TRUE, \n                               squish = TRUE, \n                               spec.name.tolower = FALSE, \n                               spec.name.nonumber = TRUE, \n                               spec.name.nobrackets = TRUE, \n                               exclude.infraspecific = FALSE, \n                               verbose = TRUE, \n                               counter = 1000)\n\nWrite csv for plant matches and save to outputs folder. This file contains all plants that had a “match” with the WFO data. Otherwise, this means that the original plant name is outdated.\n\n#write_csv(trnL_plant_match, \"../outputs/trnL_plant_match.csv\")\n\nSelect plants that had fuzzy values and select needed columns from WFO.match data output.\n\nfuzzy_plants_trnL <- trnL_plant_match %>% \n  filter(Fuzzy == 'TRUE') %>% \n  select(Fuzzy,\n         Old.name,\n         family,\n         genus,\n         Species,\n         scientificName, \n         taxonID,\n         taxonomicStatus,\n         New.accepted)\n\nKeep only distinct updated scientific name matches, and remove subspecies distinction.\n\nfuzzy_plants_trnL <- fuzzy_plants_trnL %>% \n  mutate(scientificName = replace(scientificName, scientificName == \"Diplotaxis erucoides  subsp. cossoniana\", \"Diplotaxis erucoides\")) %>% \n  distinct(scientificName, .keep_all = TRUE)\n\nAdd updated names to ITS2 data with a left join.\n\nupdated_trnL <- trnL_plants %>% \n  left_join(fuzzy_plants_trnL, by = \"Species\") \n\nRename columns and deselect unneeded columns.\n\nupdated_trnL <- updated_trnL %>% \n  rename(updated.Name = scientificName) %>% \n  rename(updated.genus = genus) %>% \n  rename(updated.family = family) %>% \n  select(-c(Fuzzy, Old.name, taxonomicStatus)) %>% \n  distinct() \n\nReplace old names, genus, and family with new names in trnL file using ifelse statement.\n\nfinal_updated_trnL <- updated_trnL %>% \n  mutate(Species = if_else(!is.na(updated.Name),\n                           true = updated.Name,\n                           false = Species)) %>% \n  mutate(Genus = if_else(!is.na(updated.genus),\n                           true = updated.genus,\n                           false = Genus)) %>% \n  mutate(Family = if_else(!is.na(updated.family),\n                          true = updated.family,\n                          false = Family))\n\nQuality check to make sure it worked!\n\nfinal_updated_trnL %>% \n  filter(Species == \"Reicheocactus famatinensis\")\n\nfinal_updated_trnL %>% \n  filter(Species == \"Lycium bridgesii\")\n\nRecreate WTUs. This is a specific identifier with a set code format used for this dataset.\n\n# Add WeeTUs for Summarizing\n# https://dplyr.tidyverse.org/reference/group_data.html\nupdated_trnL_reads_WeeTU <- final_updated_trnL %>% \n  mutate(WTU.kingdom = group_indices(., Kingdom),\n         WTU.clade1 = group_indices(., Kingdom, Clade1),\n         WTU.clade2 = group_indices(., Kingdom, Clade1, Clade2),\n         WTU.order = group_indices(., Kingdom, Clade1, Clade2, Order),\n         WTU.family = group_indices(., Kingdom, Clade1, Clade2, Order, Family),\n         WTU.subfamily = group_indices(., Kingdom, Clade1, Clade2, Order, \n                                       Family, Subfamily),\n         WTU.genus = group_indices(., Kingdom, Clade1, Clade2, Order, Family, \n                                   Subfamily, Genus),\n         WTU.species = group_indices(., Kingdom, Clade1, Clade2, Order, Family, \n                                     Subfamily, Genus, Species))\n\nRemove unneeded columns and save final data frame as a csv to outputs folder.\n\nupdated_trnL_reads_WeeTU <- updated_trnL_reads_WeeTU %>% \n  select(-c(updated.family, updated.genus, updated.Name, New.accepted))\n\n#write_csv(final_updated_trnL, \"../outputs/updated_trnL_reads_WeeTU.csv\")"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Projects",
    "section": "",
    "text": "ETL: Extract, Transform, Load\n\n\n\n\n\n\n\nPython\n\n\nSQL\n\n\nAWS\n\n\nValidation\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nBailie Wynbelt\n\n\n\n\n\n\n  \n\n\n\n\nPython - Iris Analysis and Visualization\n\n\n\n\n\n\n\nPython\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nBailie Wynbelt\n\n\n\n\n\n\n  \n\n\n\n\nData Wrangling, Updates, and Validation\n\n\n\n\n\n\n\nR\n\n\nWrangling\n\n\nValidation\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nBailie Wynbelt\n\n\n\n\n\n\n  \n\n\n\n\nR - Palmer Penguin Analysis and Visualization\n\n\n\n\n\n\n\nR\n\n\nAnalysis\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nBailie Wynbelt\n\n\n\n\n\n\nNo matching items"
  }
]